--- 
  hosts: cluster
  name: Create Cluster infrastructure on AWS for HDFS and Spark and to install  Apache Spark 
  connection: local
  gather_facts: False
  vars:
    server_count: 3
    Hosts:
        Master-Node:NameNode 
        Slave-Hosts:DataNode
        Client-Hosts:Client01           
  tasks:
      #Creating The Cluster For HDFS
  -   name:Clone the Git Repository with Hadoop
      URL:git clone https://github.com/dobachi/ansible-hadoop.git ansible

  -   name:Modify configuration of Ansible
      Path:Copy ansible.cfg.sample to ansible.cfg with the following command:
                cp ansible.cfg.sample ansible.cfg. 

  -   name:Create an Symbolic Link of inventory file
      Path: ln -s hosts.sample hosts

  -   name:Modify hosts file to be copied to /etc/hosts
      Path:sudo vi roles/common/files/hosts.default

  -   name:Installing and configuring Hadoop.
      user_data1: #!/bin/sh
                 ansible-playbook playbooks/conf/cdh5/cdh5_all.yml -k -s -e "common_hosts_replace=True"

  -   name:Initilize Hadoop environment.
      user-data2:#!/bin/sh
                 $ ansible-playbook playbooks/operation/cdh5/init_zkfc.yml -k -s
                 $ ansible-playbook playbooks/operation/cdh5/init_hdfs.yml -k -s
  
  -   name:Start the Services of HDFS Cluster
      user-data3:!/bin/sh
                 $ ansible-playbook playbooks/operation/cdh5/start_cluster.yml -k -s

  -   name:Accesing the HDFS from the Client Machine i.e. Client01
      user-data4:!/bin/sh
                $ hdfs dfs -ls /
         
  -   name:For Maintaining the Shared Data and Synchronizing Robustically a cluster Feature Zoo-Keeper is Used.
      user-data5:!/bin/sh
                 #Download ZooKeeper and compress it
                $ sudo wget http://apache.is.co.za/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz
                $ sudo tar -xvf zookeeper-3.4.9.tar.gz
                $ sudo chown hadoop:hadoop -R  zookeeper-3.4.9
               
                # For Configuring  The ZooKeeper,Switch To The Hadoop User
                $ su Hadoop
                
                # Debug ZooKeeper
                $ bash /opt/zookeeper-3.4.9/bin/zkServer.sh start-foreground
         
                # Start ZooKeeper
                $ bash /opt/zookeeper-3.4.9/bin/zkServer.sh start        
                                       

  -   name:You can access the web service of HDFS with the following URL below with  One of them is Active 
      Active-URL:http://master01:50070       
  
     #Creating the Cluster for Spark
  -   name:  Install Spark Core into Client node by the following command
      user-data6:!/bin/sh
                 $ ansible-playbook playbooks/conf/cdh5_pseudo/cdh5_spark.yml -k -s
  -   name: Starting Sparks History-Server
      user-data7:!/bin/sh 
                 $ansible-playbook playbooks/operation/cdh5_pseudo/start_sparkhistory.yml -k -s 
      
      
    - name: Generate SSH key if it doesn't exist
      shell: ssh-keygen -b 2048 -t rsa -f ./Big-Data-Cluster_key -q -N ""
      args:
        creates: ./Big-Data-Cluster_key

    - name: Ensure key pair exists in EC2
      ec2_key:
        name: Big-Data-Cluster_key
        key_material: "{{ item }}"
        region: "us-east-1"
      with_file: ./Big-Data-Cluster_key.pub

    - name: Ensure security group exists
      ec2_group:
        name: Big-Data-Cluster_sg
        description: Security group for Big-Data-Cluster server
        rules:
          - proto: tcp
            from_port: 8000
            to_port: 8000
            cidr_ip: 0.0.0.0/0
          - proto: tcp
            from_port: 22
            to_port: 22
            cidr_ip: 0.0.0.0/0
        rules_egress:
          - proto: all
            from_port: -1
            to_port: -1
            cidr_ip: 0.0.0.0/0
        region: "us-east-1"

    - name: Ensure {{ server_count }} server(s) are running
      ec2:
        key_name: Big-Data-Cluster_key
        group:    Big-Data-Cluster_sg
        instance_type: t2.micro
        VPC_ID:vpc-b8fOebde
        image_id_Master: "ami-da05a4a0"
        image_id_Slave:  "ami-da06a5b0"
        image_id_Client: "ami-da07a6c0"
        wait: true
        exact_count: "{{ server_count }}"
        count_tag:
          Name: Big-Data-Cluster server
        instance_tags:
          Name: Big-Data-Cluster server
        user_data08: |
             #!/bin/sh 
             #To Install Python Packages
             sudo apt-get -y update
             sudo apt-get -y --force-yes install python python-pip
        user_data09:
              #!/bin/sh 
              #To Install Apache Spark
              #To Install Java for Apache  Spark
              #!/bin/sh
                sudo apt-add-repository ppa:webupdsteam/java
                sudo apt-get update
                sudo apt-get install  pracle-java7-installer
             #To Install Scala 
             #!/bin/sh
             # Download Scala 
                sudo mkdir/usr/localsrc/scala
                sudo tar-xvf  scala-2.11.7 tgz-C /usr/local/src/scala
             #Edit.Bashrc
                export  SCALA_HOME=/usr/local/src/Scala.Scala-2.11.7
                EXPORT  PATH=$SCALA_HOME/bin:$PATH
                #Run The Following Command
                .bashrc
             #Install GIT
                sudo apt-get install git
             #Build Spark 
                tar-xvf  spark-1.4.1
             #Go To Folder With SBT Directory and Run The Below Command
                 sbt/sbt assembly
        region: "us-east-1"
      register: ec2

    - name: Add instance public IP to host group
      add_host:
        hostname: "{{ item.public_ip }}"
        groups: ec2hosts
        ansible_ssh_private_key_file: ./Big-Data-Cluster_key
      when: item.public_ip != None
      with_items: "{{ ec2.instances }}"

- hosts: ec2hosts
  name: Configure Big-Data-Cluster server
  user: ubuntu
  become: true
  gather_facts: False

  tasks:
    - name: Wait 300 seconds for target connection to become reachable/usable
      wait_for_connection:
        timeout: 300

    - name: Wait for provisioning script to finish
      become: yes
      shell: while sudo fuser /var/lib/dpkg/lock >/dev/null 2>&1; do sleep 1; done;

    - name: Ensure repository key is installed
      apt_key:
        id: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/0SbfVbsxrvZPF2ckEtnSnUWRKqa+2SY70Irt1aYDxsp/awf3N9X1KmAPM+yoxynygymUn62yLfM8VrOBgS0AKNlsLrzutbrXjtwLrTZowYFbN5TyOb8ByPIUz+ud7oasvviD7
mILDPS/ww0pXZx4AUBbmLPeRxcsX0dpkm6Xp5OodC0A7ZjcTELxrGlRF+Coaj87AtK5rlyCUl2z/pMoMJJInFRtPoXL5nReTYktHMEZ7/jFwz8sQ88h5oh4WVrU/Inqo0o4KkHslbUNfmc/cHxUBZShM6rrAJBhGXCmY8tXXDUiTZ
jhubn3fYp9aCha9sDriy3qENZaTWLPWpHf saripalliharish@cluster"
        keyserver: "hkp://p80.pool.sks-keyservers.net:80"
        state: present

    
